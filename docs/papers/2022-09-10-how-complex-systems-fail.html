<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">

        <title><a href="/" class="back-link">Home</a></div><h1>How Complex Systems Fail</title>
        <meta property="og:image" content="/assets/banners/2022-09-10-how-complex-systems-fail.jpg">
        <meta property="og:type" content="article">

        <link rel="icon" type="image/png" href="/assets/favicon.ico">
        <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">

        <link href="/assets/main.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
    </head>
    <body>

    <div><a href="/" class="back-link">Home</a></div><h1>How Complex Systems Fail</h1>
<p>Today&#39;s summary is about a  <a href="https://www.adaptivecapacitylabs.com/HowComplexSystemsFail.pdf">paper</a> written by Richard I. Cook (MD) in 2002.</p>
<p>The paper already succinctly conveys the key ideas.</p>
<hr>
<ol>
<li>Complex systems are intrinsically hazardous systems.</li>
<li>Complex systems are heavily and successfully defended (technically and ops-wise) against failure.</li>
<li>Catastrophe requires multiple failures – single-point failures are not enough.</li>
<li>Complex systems contain changing mixtures of failures latent within them.</li>
<li>Complex systems run in degraded mode: Corollary of (4) — complex systems run as broken systems.</li>
<li>Catastrophe is always just around the corner.</li>
<li>Post-accident attribution of an accident to a ‘root cause’ is fundamentally wrong: There are multiple contributors to accidents. Each of these is necessary insufficient in itself to create an accident.<blockquote>
<p>The evaluations based on such reasoning as ‘root cause’ do not reflect a technical understanding of the nature of failure but rather the social, and cultural need to blame specific, localized forces or events for outcomes.  </p>
</blockquote>
</li>
<li>Hindsight biases post-accident assessments of human performance: Knowledge of the outcome makes it seem that events leading to the outcome should have appeared more salient to practitioners at the time than was actually the case.</li>
<li>Human operators have dual roles: as producers &amp; as defenders against failure: The system practitioners operate the system in order to produce its desired product and also work to forestall accidents.</li>
<li>All practitioner actions are gambles: All practitioner actions are actually gambles, that is, acts that take place in the face of uncertain outcomes.</li>
<li>Actions by practitioners at the sharp end resolve all organization ambiguity.</li>
<li>Human practitioners are the adaptable element of complex systems.</li>
<li>Human expertise in complex systems is constantly changing as technology changes and people move.</li>
<li>Change introduces new forms of failure: When new technologies are used to eliminate well-understood system failures or to gain high precision performance they often introduce new pathways to large-scale, catastrophic failures. Low consequence vs frequent failures.</li>
<li>Views of ‘cause’ limit the effectiveness of defenses against future events: Instead of increasing safety, post-accident remedies usually increase the coupling and complexity of the system. Past failures are over-indexed even tho they are less likely to happen again.</li>
<li>Safety is a characteristic of systems and not of their components.</li>
<li>People continuously create safety.</li>
<li>Failure-free operations require experience with failure.</li>
</ol>


    </body>
</html>